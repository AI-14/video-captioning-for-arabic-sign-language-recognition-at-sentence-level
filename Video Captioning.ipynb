{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "In this notebook, we will create a video captioning model which uses sign-language video frames as input and outputs a translation of it in arabic. For simplicity we can translate the arabic sentences to english and then translate them back. For this assignment, we will use english sentences which were translated from original arabic captions given."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision\n",
    "from torchvision.datasets import ImageFolder\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from torchmetrics import WER\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature extraction using Vgg16\n",
    "We will extract features from both train and test using vgg16. Finally, each label will have (num_samples X 80 X 4096) features. \n",
    "Since, there are 534 samples in train and test. We will have our final df of shape (534, 80 X 4097). The last entry in 4097 is that of the label.\n",
    "When we create our custom dataset class, we will use this entry to fetch the sentence label from the groundTruth.txt file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VGG(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): ReLU(inplace=True)\n",
      "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (6): ReLU(inplace=True)\n",
      "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (8): ReLU(inplace=True)\n",
      "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): ReLU(inplace=True)\n",
      "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (13): ReLU(inplace=True)\n",
      "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (15): ReLU(inplace=True)\n",
      "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (18): ReLU(inplace=True)\n",
      "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (20): ReLU(inplace=True)\n",
      "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (22): ReLU(inplace=True)\n",
      "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (25): ReLU(inplace=True)\n",
      "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (27): ReLU(inplace=True)\n",
      "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (29): ReLU(inplace=True)\n",
      "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Dropout(p=0.5, inplace=False)\n",
      "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "    (4): ReLU(inplace=True)\n",
      "    (5): Dropout(p=0.5, inplace=False)\n",
      "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "vgg16 = torchvision.models.vgg16(pretrained=True)\n",
    "print(vgg16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGG16FeatureExtractor(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.vgg16 = torchvision.models.vgg16(pretrained=True)\n",
    "        self.fine_tune()\n",
    "    \n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Shape of x: (batch_size, channels, height, width)\n",
    "        x = self.vgg16(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "    def fine_tune(self):\n",
    "        for param in self.vgg16.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        self.vgg16.classifier = nn.Sequential(*[self.vgg16.classifier[i] for i in range(4)]) # Keeping only till classifier(3) layer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(root_dir_path, saved_feat_dir_path):\n",
    "\n",
    "    dirs = ['0001', '0002', '0003', '0004', '0005', '0006', '0007', '0008', '0009', '0010']\n",
    "\n",
    "    feature_extractor_vgg16 = VGG16FeatureExtractor()\n",
    "\n",
    "    for dir in dirs:\n",
    "        print(f'Extracting features from {root_dir_path}/{dir}')\n",
    "        \n",
    "        dirwise_feats = []\n",
    "\n",
    "        label = int(dir[-1]) \n",
    "        # NOTE: For the dir '0010', label would be 0.0 i.e. this signifies 10th label.\n",
    "        \n",
    "        transform = transforms.Compose([\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.ToTensor()\n",
    "        ])\n",
    "\n",
    "        dataset = ImageFolder(root=os.path.join(root_dir_path, dir), transform=transform)\n",
    "        loader = DataLoader(dataset, batch_size=80)\n",
    "        \n",
    "        for img, _ in tqdm(loader):\n",
    "            features = feature_extractor_vgg16(img) # Shape: (80, 4096)\n",
    "            features_np = features.numpy() # Shape: (80, 4096)\n",
    "            features_np = features_np.flatten() # Shape: (327680,)\n",
    "            features_label_np = np.append(features_np, label) # Shape: (327681,)\n",
    "            dirwise_feats.append(features_label_np)\n",
    "            \n",
    "        dirwise_feats = np.array(dirwise_feats) # Shape: (48, 327681)\n",
    "        print(f'dir {dir} dirwise_feats.shape = {dirwise_feats.shape}')\n",
    "        pd.DataFrame(dirwise_feats).to_csv(f'{saved_feat_dir_path}\\{dir}.csv', sep=',', header=None, index=None)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting features from train/0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 48/48 [06:10<00:00,  7.71s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dir 0001 dirwise_feats.shape = (48, 327681)\n",
      "Extracting features from train/0002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [06:39<00:00,  7.98s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dir 0002 dirwise_feats.shape = (50, 327681)\n",
      "Extracting features from train/0003\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 48/48 [06:23<00:00,  8.00s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dir 0003 dirwise_feats.shape = (48, 327681)\n",
      "Extracting features from train/0004\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 48/48 [06:30<00:00,  8.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dir 0004 dirwise_feats.shape = (48, 327681)\n",
      "Extracting features from train/0005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60/60 [08:04<00:00,  8.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dir 0005 dirwise_feats.shape = (60, 327681)\n",
      "Extracting features from train/0006\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 49/49 [06:32<00:00,  8.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dir 0006 dirwise_feats.shape = (49, 327681)\n",
      "Extracting features from train/0007\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 70/70 [09:12<00:00,  7.89s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dir 0007 dirwise_feats.shape = (70, 327681)\n",
      "Extracting features from train/0008\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 62/62 [07:59<00:00,  7.73s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dir 0008 dirwise_feats.shape = (62, 327681)\n",
      "Extracting features from train/0009\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 48/48 [06:05<00:00,  7.62s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dir 0009 dirwise_feats.shape = (48, 327681)\n",
      "Extracting features from train/0010\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 51/51 [06:30<00:00,  7.65s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dir 0010 dirwise_feats.shape = (51, 327681)\n"
     ]
    }
   ],
   "source": [
    "extract_features(root_dir_path='train', saved_feat_dir_path='features_train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting features from test/0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:48<00:00,  8.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dir 0001 dirwise_feats.shape = (6, 327681)\n",
      "Extracting features from test/0002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:48<00:00,  8.12s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dir 0002 dirwise_feats.shape = (6, 327681)\n",
      "Extracting features from test/0003\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:48<00:00,  8.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dir 0003 dirwise_feats.shape = (6, 327681)\n",
      "Extracting features from test/0004\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:48<00:00,  8.17s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dir 0004 dirwise_feats.shape = (6, 327681)\n",
      "Extracting features from test/0005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [01:36<00:00,  8.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dir 0005 dirwise_feats.shape = (12, 327681)\n",
      "Extracting features from test/0006\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:48<00:00,  8.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dir 0006 dirwise_feats.shape = (6, 327681)\n",
      "Extracting features from test/0007\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:48<00:00,  8.00s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dir 0007 dirwise_feats.shape = (6, 327681)\n",
      "Extracting features from test/0008\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:48<00:00,  8.08s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dir 0008 dirwise_feats.shape = (6, 327681)\n",
      "Extracting features from test/0009\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:49<00:00,  8.26s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dir 0009 dirwise_feats.shape = (6, 327681)\n",
      "Extracting features from test/0010\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:47<00:00,  7.99s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dir 0010 dirwise_feats.shape = (6, 327681)\n"
     ]
    }
   ],
   "source": [
    "extract_features(root_dir_path='test', saved_feat_dir_path='features_test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset/Dataloaders preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataframes(root_dir, name):\n",
    "    files = os.listdir(root_dir)\n",
    "    all_df = [] # A list to store all dfs so that they can be concatenated at the end.\n",
    "    for file in files:\n",
    "        df = pd.read_csv(os.path.join(root_dir, file), sep=',', header=None, engine='python')\n",
    "        all_df.append(df)\n",
    "        print(f'{file} done')\n",
    "\n",
    "    combined_df = pd.concat(all_df, axis=0)\n",
    "    print(f'{name} shape = {combined_df.shape}')\n",
    "    return combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading train data:\n",
      "0001.csv done\n",
      "0002.csv done\n",
      "0003.csv done\n",
      "0004.csv done\n",
      "0005.csv done\n",
      "0006.csv done\n",
      "0007.csv done\n",
      "0008.csv done\n",
      "0009.csv done\n",
      "0010.csv done\n",
      "train_df shape = (534, 327681)\n",
      "\n",
      "Loading test data:\n",
      "0001.csv done\n",
      "0002.csv done\n",
      "0003.csv done\n",
      "0004.csv done\n",
      "0005.csv done\n",
      "0006.csv done\n",
      "0007.csv done\n",
      "0008.csv done\n",
      "0009.csv done\n",
      "0010.csv done\n",
      "test_df shape = (66, 327681)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Takes around 7 mins to load all csv files.\n",
    "\n",
    "print('Loading train data:')\n",
    "train_df = load_dataframes(root_dir='features_train', name='train_df')\n",
    "print()\n",
    "\n",
    "print('Loading test data:')\n",
    "test_df = load_dataframes(root_dir='features_test', name='test_df')\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_captions(filepath):\n",
    "    # Add all the captions.\n",
    "    captions = []\n",
    "    f = open(filepath, \"r\", encoding='utf-8')\n",
    "    for line in f.readlines():\n",
    "        captions.append(line.split())\n",
    "\n",
    "\n",
    "    vocab = set() # Total unique words including <SOS>, <EOS>, <PAD> forms the vocab. \n",
    "    for caption in captions:\n",
    "        print(caption, f'len = {len(caption)}')\n",
    "        for token in caption:\n",
    "            vocab.add(token)\n",
    "    print(f'\\nVocab:\\n{vocab} len = {len(vocab)}')\n",
    "\n",
    "    # Mapping string/word to an index.\n",
    "    stoi = {\n",
    "    '<PAD>': 0,\n",
    "    '<SOS>': 1,\n",
    "    '<EOS>': 2,\n",
    "    }\n",
    "\n",
    "    temp = {}\n",
    "    idx = 3 # Since indices 0,1,2 are already reversed for tokens <PAD>, <SOS>, <EOS> respectively.\n",
    "    for caption in captions:\n",
    "        for tok in caption:\n",
    "            if tok not in ['<PAD>', '<SOS>', '<EOS>'] and tok not in temp:\n",
    "                temp[tok] = idx\n",
    "                idx += 1\n",
    "                \n",
    "    stoi.update(temp)\n",
    "    print(f'\\nString-to-index mapping:\\n{stoi}\\n')\n",
    "\n",
    "    # Mapping index to string/word.\n",
    "    itos = {value : key for (key, value) in stoi.items()}\n",
    "    print(f'\\nIndex-to-string mapping:\\n{itos}\\n')\n",
    "\n",
    "    return captions, stoi, itos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<SOS>', 'god', 'name', '<EOS>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>'] len = 14\n",
      "['<SOS>', 'thank', 'god', '<EOS>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>'] len = 14\n",
      "['<SOS>', 'all', 'deaf', 'arab', 'listeners', '<EOS>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>'] len = 14\n",
      "['<SOS>', 'peace', 'be', 'upon', 'you', 'may', 'gods', 'mercy', 'and', 'blessings', 'be', 'upon', 'you', '<EOS>'] len = 14\n",
      "['<SOS>', 'today', 'i', 'present', 'to', 'you', 'another', 'program', '<EOS>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>'] len = 14\n",
      "['<SOS>', 'the', 'subject', 'of', 'studying', 'arabic', 'sign', 'language', '<EOS>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>'] len = 14\n",
      "['<SOS>', 'words', 'of', 'the', 'day', 'are', 'scattered', 'in', 'religion', '<EOS>', '<PAD>', '<PAD>', '<PAD>', '<PAD>'] len = 14\n",
      "['<SOS>', 'also', 'normal', 'words', '<EOS>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>'] len = 14\n",
      "['<SOS>', 'god', 'does', 'not', 'shirk', '<EOS>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>'] len = 14\n",
      "['<SOS>', 'god', 'is', 'the', 'greatest', '<EOS>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>'] len = 14\n",
      "\n",
      "Vocab:\n",
      "{'present', 'god', 'upon', 'are', 'sign', 'normal', 'the', 'not', 'gods', '<SOS>', 'is', 'you', 'be', 'also', 'name', 'arab', 'i', 'another', 'program', 'studying', 'thank', 'in', 'mercy', '<EOS>', 'listeners', 'may', 'and', 'subject', 'of', 'day', 'to', 'peace', 'religion', 'words', 'does', 'scattered', 'greatest', '<PAD>', 'shirk', 'blessings', 'all', 'language', 'deaf', 'today', 'arabic'} len = 45\n",
      "\n",
      "String-to-index mapping:\n",
      "{'<PAD>': 0, '<SOS>': 1, '<EOS>': 2, 'god': 3, 'name': 4, 'thank': 5, 'all': 6, 'deaf': 7, 'arab': 8, 'listeners': 9, 'peace': 10, 'be': 11, 'upon': 12, 'you': 13, 'may': 14, 'gods': 15, 'mercy': 16, 'and': 17, 'blessings': 18, 'today': 19, 'i': 20, 'present': 21, 'to': 22, 'another': 23, 'program': 24, 'the': 25, 'subject': 26, 'of': 27, 'studying': 28, 'arabic': 29, 'sign': 30, 'language': 31, 'words': 32, 'day': 33, 'are': 34, 'scattered': 35, 'in': 36, 'religion': 37, 'also': 38, 'normal': 39, 'does': 40, 'not': 41, 'shirk': 42, 'is': 43, 'greatest': 44}\n",
      "\n",
      "\n",
      "Index-to-string mapping:\n",
      "{0: '<PAD>', 1: '<SOS>', 2: '<EOS>', 3: 'god', 4: 'name', 5: 'thank', 6: 'all', 7: 'deaf', 8: 'arab', 9: 'listeners', 10: 'peace', 11: 'be', 12: 'upon', 13: 'you', 14: 'may', 15: 'gods', 16: 'mercy', 17: 'and', 18: 'blessings', 19: 'today', 20: 'i', 21: 'present', 22: 'to', 23: 'another', 24: 'program', 25: 'the', 26: 'subject', 27: 'of', 28: 'studying', 29: 'arabic', 30: 'sign', 31: 'language', 32: 'words', 33: 'day', 34: 'are', 35: 'scattered', 36: 'in', 37: 'religion', 38: 'also', 39: 'normal', 40: 'does', 41: 'not', 42: 'shirk', 43: 'is', 44: 'greatest'}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "captions, stoi, itos = prepare_captions(filepath='groundTruth.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VideoCaptionDataset(Dataset):\n",
    "    def __init__(self, df, stoi, captions) -> None:\n",
    "        super().__init__()\n",
    "        self.df = df\n",
    "        self.stoi = stoi\n",
    "        self.captions = captions\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Reshaping the data to (frames, extracted_features) i.e. (80, 4096) in our case.\n",
    "        sample = self.df.iloc[index, :-1].to_numpy().reshape(80, 4096) \n",
    "        sample = torch.tensor(sample).float()\n",
    "        \n",
    "        # Label corresponds to the last column of the df. This is just a number from 1.0 - 9.0 with 0.0 signifying label 10.0\n",
    "        label = self.df.iloc[index, -1]\n",
    "        if label == 0.0:\n",
    "            label = 10.0\n",
    "        \n",
    "        tokenized_caption = self.captions[int(label) - 1]\n",
    "        \n",
    "        mapped_caption = []\n",
    "        # Convert the sentences to their mapping through stoi.\n",
    "        for tok in tokenized_caption:\n",
    "            mapped_caption.append(stoi[tok])\n",
    "        \n",
    "        return sample, torch.tensor(mapped_caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = VideoCaptionDataset(train_df, stoi, captions)\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "test_dataset = VideoCaptionDataset(test_df, stoi, captions)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder-Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda'\n",
    "\n",
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, dropout) -> None:\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, dropout=dropout)       \n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Shape of x: (frames, batch_size, input_size) i.e. (80, batch_size, 4096)\n",
    "        \n",
    "        outputs, (hidden, cell) = self.lstm(x)\n",
    "        # Shape of outputs: (80, batch_size, 512)\n",
    "        # Shape of hidden: (2, batch_size, 512)\n",
    "        # Shape of cell: (2, batch_size, 512)\n",
    "\n",
    "        return hidden, cell\n",
    "\n",
    "\n",
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, num_embeddings, embedding_dim, input_size, hidden_size, num_layers, dropout, vocab_size) -> None:\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(num_embeddings=num_embeddings, embedding_dim=embedding_dim)\n",
    "        self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, dropout=dropout)\n",
    "        self.fc = nn.Linear(in_features=hidden_size, out_features=vocab_size)\n",
    "        \n",
    "        \n",
    "    def forward(self, captions, enc_hidden, enc_cell):\n",
    "        # We will give decoder one word at a time => seq_len = 1\n",
    "\n",
    "        # Shape of captions: (batch_size,). But we need to create a sequence. Hence, we will shape it into (1, batch_size)\n",
    "        # Shape of enc_hidden: (2, batch_size, 512)\n",
    "        # Shape of enc_cell: (2, batch_size, 512)\n",
    "\n",
    "        captions = captions.unsqueeze(0) # Shaping into (1, batch_size)\n",
    "        \n",
    "        embeddings = self.embed(captions)\n",
    "        # Shape of embeddings: (seq_len, batch_size, embedding_dim)\n",
    "\n",
    "        outputs, (hidden, cell) = self.lstm(embeddings, (enc_hidden, enc_cell)) # We pass the enc_hidden & enc_cell to the hidden states of our decoder as initial states\n",
    "        # Shape of outputs: (1, batch_size, hidden_size) i.e. (1, batch_size, 512)\n",
    "        # Shape of hidden: (num_layers, batch_size, hidden_size) i.e. (2, batch_size, 512)\n",
    "        # Shape of cell: (num_layers, batch_size, hidden_size) i.e. (2, batch_size, 512)\n",
    "\n",
    "        predictions = self.fc(outputs)\n",
    "        # Shape of predictions: (1, batch_size, vocab_size) i.e. (1, batch_size, 45)\n",
    "        # But these outputs will go to softmax and softmax expects (batch_size, classes) i.e. (batch_size, 45)\n",
    "        # Therefore, we need to remove the first dimentsion i.e. 1 to make preditions shape to be (batch_size, 45)\n",
    "        predictions = predictions.squeeze(0)\n",
    "\n",
    "        return predictions, hidden, cell\n",
    "\n",
    "\n",
    "class EncoderDecoderModel(nn.Module):\n",
    "    def __init__(self, encoder, decoder, vocab_size) -> None:\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.vocab_size = vocab_size\n",
    "\n",
    "\n",
    "    def forward(self, img_frames, captions, teacher_force_ratio=0.5):\n",
    "        # Shape of img_frames: (frames, batch_size, input_size) i.e. (80, batch_size, 4096)\n",
    "        # Shape of captions: (seq_len, batch_size) i.e. (14, batch_size)\n",
    "\n",
    "        seq_len, batch_size = captions.size()\n",
    "        \n",
    "        hidden, cell = self.encoder(img_frames)\n",
    "\n",
    "        outputs = torch.zeros(seq_len, batch_size, self.vocab_size).to(device)\n",
    "        \n",
    "        x = captions[0] # Grab the start token in the batch i.e. the <SOS> token whose index is 1.\n",
    "        \n",
    "        for t in range(1, seq_len):\n",
    "            # Use previous hidden, cell as context from encoder at start i.e. use enc_hidden & enc_cell.\n",
    "            predictions, hidden, cell = self.decoder(x, hidden, cell)\n",
    "\n",
    "            # Store the prediction.\n",
    "            outputs[t] = predictions\n",
    "\n",
    "            # Get the best word the decoder predicted (index in the vocabulary)\n",
    "            best_guess = predictions.argmax(1)\n",
    "           \n",
    "            # With probability of teacher_force_ratio we take the actual next word\n",
    "            # otherwise we take the word that the Decoder predicted it to be.\n",
    "            x = captions[t] if random.random() < teacher_force_ratio else best_guess\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Running the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def start_training(train_loader):\n",
    "    LR = 1e-4\n",
    "    WD = 1e-4\n",
    "    PATIENCE = 5\n",
    "    EPOCHS = 10\n",
    "\n",
    "    # Inputs for the encoder, decoder & encoder-decoder combined model.\n",
    "    input_size_encoder = 4096\n",
    "    hidden_size = 512\n",
    "    num_layers = 2\n",
    "    dropout_encoder = 0.4\n",
    "    num_embeddings = 45\n",
    "    embedding_dim = 300\n",
    "    input_size_decoder = 300 \n",
    "    dropout_decoder = 0.1\n",
    "    vocab_size = 45\n",
    "\n",
    "    encoder = EncoderRNN(input_size_encoder, hidden_size, num_layers, dropout_encoder).to(device)\n",
    "    decoder = DecoderRNN(num_embeddings, embedding_dim, input_size_decoder, hidden_size, num_layers, dropout_decoder, vocab_size).to(device)\n",
    "    encoder_decoder = EncoderDecoderModel(encoder, decoder, vocab_size).to(device)\n",
    "\n",
    "    optimizer = optim.Adam(encoder_decoder.parameters(), lr=LR, weight_decay=WD)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.01, patience=PATIENCE, verbose=True)\n",
    "\n",
    "    loss_fn = nn.CrossEntropyLoss(ignore_index=0) # We ignore the index of <PAD> which is 0.\n",
    "\n",
    "\n",
    "    # Training phase.\n",
    "    for epoch in range(1, EPOCHS+1):\n",
    "        encoder_decoder.train()\n",
    "        \n",
    "        running_loss = 0.0\n",
    "    \n",
    "        prog_bar = tqdm(enumerate(train_loader), total=len(train_loader), leave=True)\n",
    "        for batch_idx, (img_frames, captions) in prog_bar:   \n",
    "            optimizer.zero_grad() \n",
    "            \n",
    "            img_frames = img_frames.permute(1, 0, 2) # Reshape into (frames, batch_size, features) i.e. (80, batch_size, 4096)\n",
    "            captions = captions.permute(1, 0) # Reshap into (seq_len, batch_size) i.e. (14, batch_size)\n",
    "\n",
    "            img_frames, captions = img_frames.to(device), captions.long().to(device)\n",
    "            \n",
    "            predicted_captions = encoder_decoder(img_frames, captions)\n",
    "            # Shape of predicted_captions is (seq_len, batch_size, vocab_size) i.e. (14, batch_size, 45)\n",
    "            \n",
    "            # We dont want the <SOS> token. Hence, we take from the first word/token.\n",
    "            predicted_captions = predicted_captions[1:].reshape(-1, predicted_captions.shape[2])\n",
    "            captions = captions[1:].reshape(-1)\n",
    "\n",
    "            loss = loss_fn(predicted_captions, captions) \n",
    "            \n",
    "            running_loss += loss.item()\n",
    "        \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            prog_bar.set_description(f'Epoch {epoch}/{EPOCHS}')\n",
    "        \n",
    "        tr_loss = running_loss / len(train_loader)\n",
    "        scheduler.step(tr_loss)\n",
    "        print(f'\\ttrain_loss = {tr_loss:.6f}')\n",
    "    \n",
    "    return encoder_decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10: 100%|██████████| 34/34 [00:03<00:00,  9.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\ttrain_loss = 3.449114\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10: 100%|██████████| 34/34 [00:03<00:00, 10.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\ttrain_loss = 2.711940\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10: 100%|██████████| 34/34 [00:03<00:00, 10.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\ttrain_loss = 2.005793\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10: 100%|██████████| 34/34 [00:03<00:00, 10.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\ttrain_loss = 1.505999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10: 100%|██████████| 34/34 [00:03<00:00, 10.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\ttrain_loss = 1.010592\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10: 100%|██████████| 34/34 [00:03<00:00, 10.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\ttrain_loss = 0.687729\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10: 100%|██████████| 34/34 [00:03<00:00, 10.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\ttrain_loss = 0.487499\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10: 100%|██████████| 34/34 [00:03<00:00, 10.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\ttrain_loss = 0.316386\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10: 100%|██████████| 34/34 [00:03<00:00, 10.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\ttrain_loss = 0.229112\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|██████████| 34/34 [00:03<00:00, 10.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\ttrain_loss = 0.180230\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model = start_training(train_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing the model and getting the word-error-rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_special_tokens(sentence):\n",
    "    cleaned_sent = ''\n",
    "    tokenized_sent = sentence.split()\n",
    "    for tok in tokenized_sent:\n",
    "        if tok not in ['<PAD>', '<SOS>', '<EOS>']:\n",
    "            cleaned_sent += tok + ' '\n",
    "    return cleaned_sent\n",
    "\n",
    "\n",
    "def decode_sentence(sentence):\n",
    "    sent = ''\n",
    "    for idx in sentence:\n",
    "        if isinstance(idx, torch.Tensor):\n",
    "            sent += itos[idx.item()] + ' '\n",
    "        else:\n",
    "            sent += itos[idx] + ' '\n",
    "    \n",
    "    sent = remove_special_tokens(sent)\n",
    "    return sent\n",
    "\n",
    "\n",
    "def get_caption(model, img_frame, true_caption):\n",
    "    # Shape of img_frame: (frames, features) i.e. (80, 4090). Hence, we will reshape it into (80, 1, 4090)\n",
    "    # Shape of true_caption: (seq_len,) i.e. (14,). Hence, we will reshape it into (14, 1)\n",
    "\n",
    "    img_frame = img_frame.unsqueeze(1) # Now shape becomes (80, 1, 4090)\n",
    "    true_caption = true_caption.unsqueeze(1) # Now shape becomes (14, 1)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        img_frame, true_caption = img_frame.to(device), true_caption.long().to(device)\n",
    "\n",
    "        hidden, cell = model.encoder(img_frame)\n",
    "\n",
    "        seq_len, batch_size = true_caption.size()\n",
    "\n",
    "        predicted_caption = [stoi['<SOS>']]\n",
    "        for t in range(1, seq_len):\n",
    "            x = torch.LongTensor([predicted_caption[-1]]).to(device)\n",
    "\n",
    "            prediction, hidden, cell = model.decoder(x, hidden, cell)\n",
    "\n",
    "            best_guess = prediction.argmax(1).item()\n",
    "            predicted_caption.append(best_guess)\n",
    "\n",
    "            if best_guess == stoi['<EOS>']:\n",
    "                break\n",
    "\n",
    "    predicted_caption_decoded = decode_sentence(predicted_caption)\n",
    "    return predicted_caption_decoded\n",
    "\n",
    "\n",
    "def test_the_model(test_loader, model):\n",
    "    wer = WER()\n",
    "\n",
    "    all_predicted = []\n",
    "    all_true = []\n",
    "\n",
    "    for batch_idx, (img_frames, captions) in enumerate(test_loader):\n",
    "        for img_f, cap in zip(img_frames, captions):\n",
    "            predicted_caption = get_caption(model, img_f, cap)\n",
    "            true_caption = decode_sentence(cap)\n",
    "\n",
    "            all_true.append(true_caption)\n",
    "            all_predicted.append(predicted_caption)\n",
    "    \n",
    "    for true_c, pred_c in zip(all_true, all_predicted):\n",
    "        print(f'ACTUAL:    {true_c}')\n",
    "        print(f'PREDICTED: {pred_c}')\n",
    "        print()\n",
    "    \n",
    "    error_rate = wer(all_predicted, all_true).item()\n",
    "    print(f'WER = {error_rate}') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACTUAL:    words of the day are scattered in religion \n",
      "PREDICTED: the subject of studying arabic sign language \n",
      "\n",
      "ACTUAL:    today i present to you another program \n",
      "PREDICTED: today i present to you another program \n",
      "\n",
      "ACTUAL:    today i present to you another program \n",
      "PREDICTED: today i present to you another program \n",
      "\n",
      "ACTUAL:    god does not shirk \n",
      "PREDICTED: the subject of studying arabic sign language \n",
      "\n",
      "ACTUAL:    god is the greatest \n",
      "PREDICTED: the subject of studying arabic sign language \n",
      "\n",
      "ACTUAL:    all deaf arab listeners \n",
      "PREDICTED: words of the day are scattered in religion \n",
      "\n",
      "ACTUAL:    all deaf arab listeners \n",
      "PREDICTED: words of the day are scattered in religion \n",
      "\n",
      "ACTUAL:    god name \n",
      "PREDICTED: today i present to you another program \n",
      "\n",
      "ACTUAL:    peace be upon you may gods mercy and blessings be upon you \n",
      "PREDICTED: words of the day are scattered in religion \n",
      "\n",
      "ACTUAL:    god name \n",
      "PREDICTED: peace be upon you may gods mercy and blessings be upon you \n",
      "\n",
      "ACTUAL:    god name \n",
      "PREDICTED: peace be upon you may gods mercy and blessings be upon you \n",
      "\n",
      "ACTUAL:    today i present to you another program \n",
      "PREDICTED: today i present to you another program \n",
      "\n",
      "ACTUAL:    also normal words \n",
      "PREDICTED: the subject of studying arabic sign language \n",
      "\n",
      "ACTUAL:    god is the greatest \n",
      "PREDICTED: god does not shirk \n",
      "\n",
      "ACTUAL:    all deaf arab listeners \n",
      "PREDICTED: today i present to you another program \n",
      "\n",
      "ACTUAL:    thank god \n",
      "PREDICTED: today i present to you another program \n",
      "\n",
      "ACTUAL:    today i present to you another program \n",
      "PREDICTED: today i present to you another program \n",
      "\n",
      "ACTUAL:    the subject of studying arabic sign language \n",
      "PREDICTED: the subject of studying arabic sign language \n",
      "\n",
      "ACTUAL:    god name \n",
      "PREDICTED: also normal words \n",
      "\n",
      "ACTUAL:    god name \n",
      "PREDICTED: peace be upon you may gods mercy and blessings be upon you \n",
      "\n",
      "ACTUAL:    thank god \n",
      "PREDICTED: today i present to you another program \n",
      "\n",
      "ACTUAL:    today i present to you another program \n",
      "PREDICTED: the subject of studying arabic sign language \n",
      "\n",
      "ACTUAL:    thank god \n",
      "PREDICTED: today i present to you another program \n",
      "\n",
      "ACTUAL:    also normal words \n",
      "PREDICTED: the subject of studying arabic sign language \n",
      "\n",
      "ACTUAL:    words of the day are scattered in religion \n",
      "PREDICTED: words of the day are scattered in religion \n",
      "\n",
      "ACTUAL:    peace be upon you may gods mercy and blessings be upon you \n",
      "PREDICTED: words of the day are scattered in religion \n",
      "\n",
      "ACTUAL:    also normal words \n",
      "PREDICTED: the subject of studying arabic sign language \n",
      "\n",
      "ACTUAL:    god does not shirk \n",
      "PREDICTED: today i present to you another program \n",
      "\n",
      "ACTUAL:    the subject of studying arabic sign language \n",
      "PREDICTED: the subject of studying arabic sign language \n",
      "\n",
      "ACTUAL:    god does not shirk \n",
      "PREDICTED: the subject of studying arabic sign language \n",
      "\n",
      "ACTUAL:    god name \n",
      "PREDICTED: also normal words \n",
      "\n",
      "ACTUAL:    also normal words \n",
      "PREDICTED: words of the day are scattered in religion \n",
      "\n",
      "ACTUAL:    the subject of studying arabic sign language \n",
      "PREDICTED: the subject of studying arabic sign language \n",
      "\n",
      "ACTUAL:    today i present to you another program \n",
      "PREDICTED: today i present to you another program \n",
      "\n",
      "ACTUAL:    god is the greatest \n",
      "PREDICTED: god is the greatest \n",
      "\n",
      "ACTUAL:    peace be upon you may gods mercy and blessings be upon you \n",
      "PREDICTED: the subject of studying arabic sign language \n",
      "\n",
      "ACTUAL:    today i present to you another program \n",
      "PREDICTED: the subject of studying arabic sign language \n",
      "\n",
      "ACTUAL:    today i present to you another program \n",
      "PREDICTED: today i present to you another program \n",
      "\n",
      "ACTUAL:    god is the greatest \n",
      "PREDICTED: the subject of studying arabic sign language \n",
      "\n",
      "ACTUAL:    god is the greatest \n",
      "PREDICTED: words of the day are scattered in religion \n",
      "\n",
      "ACTUAL:    words of the day are scattered in religion \n",
      "PREDICTED: words of the day are scattered in religion \n",
      "\n",
      "ACTUAL:    today i present to you another program \n",
      "PREDICTED: today i present to you another program \n",
      "\n",
      "ACTUAL:    god is the greatest \n",
      "PREDICTED: god is the greatest \n",
      "\n",
      "ACTUAL:    also normal words \n",
      "PREDICTED: the subject of studying arabic sign language \n",
      "\n",
      "ACTUAL:    the subject of studying arabic sign language \n",
      "PREDICTED: the subject of studying arabic sign language \n",
      "\n",
      "ACTUAL:    today i present to you another program \n",
      "PREDICTED: today i present to you another program \n",
      "\n",
      "ACTUAL:    words of the day are scattered in religion \n",
      "PREDICTED: words of the day are scattered in religion \n",
      "\n",
      "ACTUAL:    all deaf arab listeners \n",
      "PREDICTED: words of the day are scattered in religion \n",
      "\n",
      "ACTUAL:    today i present to you another program \n",
      "PREDICTED: the subject of studying arabic sign language \n",
      "\n",
      "ACTUAL:    the subject of studying arabic sign language \n",
      "PREDICTED: today i present to you another program \n",
      "\n",
      "ACTUAL:    the subject of studying arabic sign language \n",
      "PREDICTED: the subject of studying arabic sign language \n",
      "\n",
      "ACTUAL:    also normal words \n",
      "PREDICTED: the subject of studying arabic sign language \n",
      "\n",
      "ACTUAL:    god does not shirk \n",
      "PREDICTED: the subject of studying arabic sign language \n",
      "\n",
      "ACTUAL:    thank god \n",
      "PREDICTED: today i present to you another program \n",
      "\n",
      "ACTUAL:    thank god \n",
      "PREDICTED: today i present to you another program \n",
      "\n",
      "ACTUAL:    today i present to you another program \n",
      "PREDICTED: today i present to you another program \n",
      "\n",
      "ACTUAL:    words of the day are scattered in religion \n",
      "PREDICTED: the subject of studying arabic sign language \n",
      "\n",
      "ACTUAL:    all deaf arab listeners \n",
      "PREDICTED: words of the day are scattered in religion \n",
      "\n",
      "ACTUAL:    peace be upon you may gods mercy and blessings be upon you \n",
      "PREDICTED: today i present to you another program \n",
      "\n",
      "ACTUAL:    god does not shirk \n",
      "PREDICTED: the subject of studying arabic sign language \n",
      "\n",
      "ACTUAL:    peace be upon you may gods mercy and blessings be upon you \n",
      "PREDICTED: today i present to you another program \n",
      "\n",
      "ACTUAL:    peace be upon you may gods mercy and blessings be upon you \n",
      "PREDICTED: the subject of studying arabic sign language \n",
      "\n",
      "ACTUAL:    words of the day are scattered in religion \n",
      "PREDICTED: words of the day are scattered in religion \n",
      "\n",
      "ACTUAL:    all deaf arab listeners \n",
      "PREDICTED: the subject of studying arabic sign language \n",
      "\n",
      "ACTUAL:    thank god \n",
      "PREDICTED: the subject of studying arabic sign language \n",
      "\n",
      "ACTUAL:    god does not shirk \n",
      "PREDICTED: today i present to you another program \n",
      "\n",
      "WER = 1.005555510520935\n"
     ]
    }
   ],
   "source": [
    "test_the_model(test_loader, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A WER of 0 can be acheived if trained for more epochs. Since, there are only 10 sentences, our model can easily predict these sentences. The complexity in the 10 sentences is also not much. Therefore, a metric of WER is useful when we have 50-100 unique captions. For example, if we train the model for 20 epochs, it will obviosuly learn all the patterns and we will get WER = 0%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder decoder with attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda'\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, hidden_size) -> None:\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.attention = torch.nn.Linear(self.hidden_size, self.hidden_size)\n",
    "    \n",
    "\n",
    "    def forward(self, hidden, enc_outputs):\n",
    "        energy = self.attention(enc_outputs)\n",
    "        attention_energy = torch.sum(hidden * energy, dim=2)\n",
    "        attention_energy = attention_energy.t() # Transposing the attention_energy tensor.\n",
    "        softmax_scores = nn.Softmax(dim=1)(attention_energy).unsqueeze(1)\n",
    "        return softmax_scores\n",
    "\n",
    "\n",
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, dropout) -> None:\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, dropout=dropout)       \n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Shape of x: (frames, batch_size, input_size) i.e. (80, batch_size, 4096)\n",
    "        \n",
    "        outputs, (hidden, cell) = self.lstm(x)\n",
    "        # Shape of outputs: (80, batch_size, 512)\n",
    "        # Shape of hidden: (2, batch_size, 512)\n",
    "        # Shape of cell: (2, batch_size, 512)\n",
    "\n",
    "        return hidden, cell\n",
    "\n",
    "\n",
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, num_embeddings, embedding_dim, input_size, hidden_size, num_layers, dropout, vocab_size) -> None:\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(num_embeddings=num_embeddings, embedding_dim=embedding_dim)\n",
    "        self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, dropout=dropout)\n",
    "        self.concat = nn.Linear(hidden_size*2, hidden_size)\n",
    "        self.fc = nn.Linear(in_features=hidden_size, out_features=vocab_size)\n",
    "        \n",
    "        self.attention_model = Attention(hidden_size)\n",
    "        \n",
    "\n",
    "    def forward(self, captions, enc_hidden, enc_cell):\n",
    "        # We will give decoder one word at a time => seq_len = 1\n",
    "\n",
    "        # Shape of captions: (batch_size,). But we need to create a sequence. Hence, we will shape it into (1, batch_size)\n",
    "        # Shape of enc_hidden: (2, batch_size, 512)\n",
    "        # Shape of enc_cell: (2, batch_size, 512)\n",
    "\n",
    "        captions = captions.unsqueeze(0) # Shaping into (1, batch_size)\n",
    "        \n",
    "        embeddings = self.embed(captions)\n",
    "        # Shape of embeddings: (seq_len, batch_size, embedding_dim)\n",
    "\n",
    "        outputs, (hidden, cell) = self.lstm(embeddings, (enc_hidden, enc_cell)) # We pass the enc_hidden & enc_cell to the hidden states of our decoder as initial states\n",
    "        # Shape of outputs: (1, batch_size, hidden_size) i.e. (1, batch_size, 512)\n",
    "        # Shape of hidden: (num_layers, batch_size, hidden_size) i.e. (2, batch_size, 512)\n",
    "        # Shape of cell: (num_layers, batch_size, hidden_size) i.e. (2, batch_size, 512)\n",
    "\n",
    "        attention_weights = self.attention_model(outputs, enc_hidden)\n",
    "        context = attention_weights.bmm(enc_hidden.transpose(0,1))\n",
    "        outputs = outputs.squeeze(0)\n",
    "        context = context.squeeze(1)\n",
    "        concat_input =  torch.cat((outputs, context), 1)\n",
    "        concat_output = torch.tanh(self.concat(concat_input))\n",
    "\n",
    "        predictions = self.fc(concat_output)\n",
    "        predictions = predictions.squeeze(0)\n",
    "        # Shape of predictions: (vocab_size) i.e. (45,)\n",
    "        \n",
    "        return predictions, hidden, cell\n",
    "\n",
    "\n",
    "class EncoderDecoderModel(nn.Module):\n",
    "    def __init__(self, encoder, decoder, vocab_size) -> None:\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.vocab_size = vocab_size\n",
    "\n",
    "\n",
    "    def forward(self, img_frames, captions, teacher_force_ratio=0.5):\n",
    "        # Shape of img_frames: (frames, batch_size, input_size) i.e. (80, batch_size, 4096)\n",
    "        # Shape of captions: (seq_len, batch_size) i.e. (14, batch_size)\n",
    "\n",
    "        seq_len, batch_size = captions.size()\n",
    "        \n",
    "        hidden, cell = self.encoder(img_frames)\n",
    "\n",
    "        outputs = torch.zeros(seq_len, batch_size, self.vocab_size).to(device)\n",
    "        \n",
    "        x = captions[0] # Grab the start token in the batch i.e. the <SOS> token whose index is 1.\n",
    "        \n",
    "        for t in range(1, seq_len):\n",
    "            # Use previous hidden, cell as context from encoder at start i.e. use enc_hidden & enc_cell.\n",
    "            predictions, hidden, cell = self.decoder(x, hidden, cell)\n",
    "\n",
    "            # Store the prediction.\n",
    "            outputs[t] = predictions\n",
    "\n",
    "            # Get the best word the decoder predicted (index in the vocabulary)\n",
    "            best_guess = predictions.argmax(1)\n",
    "           \n",
    "            # With probability of teacher_force_ratio we take the actual next word\n",
    "            # otherwise we take the word that the Decoder predicted it to be.\n",
    "            x = captions[t] if random.random() < teacher_force_ratio else best_guess\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Running the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def start_training(train_loader):\n",
    "    LR = 1e-4\n",
    "    WD = 1e-4\n",
    "    PATIENCE = 5\n",
    "    EPOCHS = 10\n",
    "\n",
    "    # Inputs for the encoder, decoder & encoder-decoder combined model.\n",
    "    input_size_encoder = 4096\n",
    "    hidden_size = 512\n",
    "    num_layers = 2\n",
    "    dropout_encoder = 0.4\n",
    "    num_embeddings = 45\n",
    "    embedding_dim = 300\n",
    "    input_size_decoder = 300 \n",
    "    dropout_decoder = 0.1\n",
    "    vocab_size = 45\n",
    "\n",
    "    encoder = EncoderRNN(input_size_encoder, hidden_size, num_layers, dropout_encoder).to(device)\n",
    "    decoder = DecoderRNN(num_embeddings, embedding_dim, input_size_decoder, hidden_size, num_layers, dropout_decoder, vocab_size).to(device)\n",
    "    encoder_decoder = EncoderDecoderModel(encoder, decoder, vocab_size).to(device)\n",
    "\n",
    "    optimizer = optim.Adam(encoder_decoder.parameters(), lr=LR, weight_decay=WD)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.01, patience=PATIENCE, verbose=True)\n",
    "\n",
    "    loss_fn = nn.CrossEntropyLoss(ignore_index=0) # We ignore the index of <PAD> which is 0.\n",
    "\n",
    "\n",
    "    # Training phase.\n",
    "    for epoch in range(1, EPOCHS+1):\n",
    "        encoder_decoder.train()\n",
    "        \n",
    "        running_loss = 0.0\n",
    "    \n",
    "        prog_bar = tqdm(enumerate(train_loader), total=len(train_loader), leave=True)\n",
    "        for batch_idx, (img_frames, captions) in prog_bar:   \n",
    "            optimizer.zero_grad() \n",
    "            \n",
    "            img_frames = img_frames.permute(1, 0, 2) # Reshape into (frames, batch_size, features) i.e. (80, batch_size, 4096)\n",
    "            captions = captions.permute(1, 0) # Reshap into (seq_len, batch_size) i.e. (14, batch_size)\n",
    "\n",
    "            img_frames, captions = img_frames.to(device), captions.long().to(device)\n",
    "            \n",
    "            predicted_captions = encoder_decoder(img_frames, captions)\n",
    "            \n",
    "            # We dont want the <SOS> token. Hence, we take from the first word/token.\n",
    "            predicted_captions = predicted_captions[1:].reshape(-1, predicted_captions.shape[2])\n",
    "            captions = captions[1:].reshape(-1)\n",
    "\n",
    "            loss = loss_fn(predicted_captions, captions) \n",
    "            \n",
    "            running_loss += loss.item()\n",
    "        \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            prog_bar.set_description(f'Epoch {epoch}/{EPOCHS}')\n",
    "        \n",
    "        tr_loss = running_loss / len(train_loader)\n",
    "        scheduler.step(tr_loss)\n",
    "        print(f'\\ttrain_loss = {tr_loss:.6f}')\n",
    "    \n",
    "    return encoder_decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10: 100%|██████████| 34/34 [00:03<00:00,  9.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\ttrain_loss = 3.293704\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10: 100%|██████████| 34/34 [00:03<00:00,  9.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\ttrain_loss = 2.525308\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10: 100%|██████████| 34/34 [00:03<00:00,  9.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\ttrain_loss = 1.866477\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10: 100%|██████████| 34/34 [00:03<00:00,  9.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\ttrain_loss = 1.245438\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10: 100%|██████████| 34/34 [00:03<00:00,  9.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\ttrain_loss = 0.891080\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10: 100%|██████████| 34/34 [00:03<00:00,  9.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\ttrain_loss = 0.598030\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10: 100%|██████████| 34/34 [00:03<00:00,  9.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\ttrain_loss = 0.379191\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10: 100%|██████████| 34/34 [00:03<00:00,  9.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\ttrain_loss = 0.285126\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10: 100%|██████████| 34/34 [00:03<00:00,  9.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\ttrain_loss = 0.212800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|██████████| 34/34 [00:03<00:00,  9.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\ttrain_loss = 0.150472\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model = start_training(train_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing the model and getting word-error rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_special_tokens(sentence):\n",
    "    cleaned_sent = ''\n",
    "    tokenized_sent = sentence.split()\n",
    "    for tok in tokenized_sent:\n",
    "        if tok not in ['<PAD>', '<SOS>', '<EOS>']:\n",
    "            cleaned_sent += tok + ' '\n",
    "    return cleaned_sent\n",
    "\n",
    "\n",
    "def decode_sentence(sentence):\n",
    "    sent = ''\n",
    "    for idx in sentence:\n",
    "        if isinstance(idx, torch.Tensor):\n",
    "            sent += itos[idx.item()] + ' '\n",
    "        else:\n",
    "            sent += itos[idx] + ' '\n",
    "    \n",
    "    sent = remove_special_tokens(sent)\n",
    "    return sent\n",
    "\n",
    "\n",
    "def get_caption(model, img_frame, true_caption):\n",
    "    # Shape of img_frame: (frames, features) i.e. (80, 4090). Hence, we will reshape it into (80, 1, 4090)\n",
    "    # Shape of true_caption: (seq_len,) i.e. (14,). Hence, we will reshape it into (14, 1)\n",
    "\n",
    "    img_frame = img_frame.unsqueeze(1) # Now shape becomes (80, 1, 4090)\n",
    "    true_caption = true_caption.unsqueeze(1) # Now shape becomes (14, 1)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        img_frame, true_caption = img_frame.to(device), true_caption.long().to(device)\n",
    "\n",
    "        hidden, cell = model.encoder(img_frame)\n",
    "\n",
    "        seq_len, batch_size = true_caption.size()\n",
    "\n",
    "        predicted_caption = [stoi['<SOS>']]\n",
    "        for t in range(1, seq_len):\n",
    "            x = torch.LongTensor([predicted_caption[-1]]).to(device)\n",
    "\n",
    "            prediction, hidden, cell = model.decoder(x, hidden, cell)\n",
    "            \n",
    "            best_guess = prediction.argmax(0).item()\n",
    "            predicted_caption.append(best_guess)\n",
    "\n",
    "            if best_guess == stoi['<EOS>']:\n",
    "                break\n",
    "\n",
    "    predicted_caption_decoded = decode_sentence(predicted_caption)\n",
    "    return predicted_caption_decoded\n",
    "\n",
    "\n",
    "def test_the_model(test_loader, model):\n",
    "    wer = WER()\n",
    "\n",
    "    all_predicted = []\n",
    "    all_true = []\n",
    "\n",
    "    for batch_idx, (img_frames, captions) in enumerate(test_loader):\n",
    "        for img_f, cap in zip(img_frames, captions):\n",
    "            predicted_caption = get_caption(model, img_f, cap)\n",
    "            true_caption = decode_sentence(cap)\n",
    "\n",
    "            all_true.append(true_caption)\n",
    "            all_predicted.append(predicted_caption)\n",
    "    \n",
    "    for true_c, pred_c in zip(all_true, all_predicted):\n",
    "        print(f'ACTUAL:    {true_c}')\n",
    "        print(f'PREDICTED: {pred_c}')\n",
    "        print()\n",
    "    \n",
    "    error_rate = wer(all_predicted, all_true).item()\n",
    "    print(f'WER = {error_rate}') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACTUAL:    today i present to you another program \n",
      "PREDICTED: today i present to you another program \n",
      "\n",
      "ACTUAL:    today i present to you another program \n",
      "PREDICTED: today i present to you another program \n",
      "\n",
      "ACTUAL:    all deaf arab listeners \n",
      "PREDICTED: words of the day are scattered in religion \n",
      "\n",
      "ACTUAL:    peace be upon you may gods mercy and blessings be upon you \n",
      "PREDICTED: words of the day are scattered in religion \n",
      "\n",
      "ACTUAL:    god name \n",
      "PREDICTED: peace be upon you may gods mercy and blessings be upon you \n",
      "\n",
      "ACTUAL:    god name \n",
      "PREDICTED: peace be upon you may gods mercy and blessings be upon you \n",
      "\n",
      "ACTUAL:    thank god \n",
      "PREDICTED: today i present to you another program \n",
      "\n",
      "ACTUAL:    all deaf arab listeners \n",
      "PREDICTED: words of the day are scattered in religion \n",
      "\n",
      "ACTUAL:    also normal words \n",
      "PREDICTED: the subject of studying arabic sign language \n",
      "\n",
      "ACTUAL:    god name \n",
      "PREDICTED: all deaf arab listeners \n",
      "\n",
      "ACTUAL:    peace be upon you may gods mercy and blessings be upon you \n",
      "PREDICTED: today i present to you another program \n",
      "\n",
      "ACTUAL:    words of the day are scattered in religion \n",
      "PREDICTED: words of the day are scattered in religion \n",
      "\n",
      "ACTUAL:    god does not shirk \n",
      "PREDICTED: the subject of studying arabic sign language \n",
      "\n",
      "ACTUAL:    god is the greatest \n",
      "PREDICTED: words of the day are scattered in religion \n",
      "\n",
      "ACTUAL:    words of the day are scattered in religion \n",
      "PREDICTED: the subject of studying arabic sign language \n",
      "\n",
      "ACTUAL:    all deaf arab listeners \n",
      "PREDICTED: words of the day are scattered in religion \n",
      "\n",
      "ACTUAL:    peace be upon you may gods mercy and blessings be upon you \n",
      "PREDICTED: words of the day are scattered in religion \n",
      "\n",
      "ACTUAL:    today i present to you another program \n",
      "PREDICTED: today i present to you another program \n",
      "\n",
      "ACTUAL:    god name \n",
      "PREDICTED: all deaf arab listeners \n",
      "\n",
      "ACTUAL:    today i present to you another program \n",
      "PREDICTED: today i present to you another program \n",
      "\n",
      "ACTUAL:    today i present to you another program \n",
      "PREDICTED: today i present to you another program \n",
      "\n",
      "ACTUAL:    god is the greatest \n",
      "PREDICTED: words of the day are scattered in religion \n",
      "\n",
      "ACTUAL:    peace be upon you may gods mercy and blessings be upon you \n",
      "PREDICTED: words of the day are scattered in religion \n",
      "\n",
      "ACTUAL:    words of the day are scattered in religion \n",
      "PREDICTED: words of the day are scattered in religion \n",
      "\n",
      "ACTUAL:    peace be upon you may gods mercy and blessings be upon you \n",
      "PREDICTED: today i present to you another program \n",
      "\n",
      "ACTUAL:    god is the greatest \n",
      "PREDICTED: words of the day are scattered in religion \n",
      "\n",
      "ACTUAL:    today i present to you another program \n",
      "PREDICTED: today i present to you another program \n",
      "\n",
      "ACTUAL:    also normal words \n",
      "PREDICTED: the of of studying arabic sign language \n",
      "\n",
      "ACTUAL:    today i present to you another program \n",
      "PREDICTED: today i present to you another program \n",
      "\n",
      "ACTUAL:    the subject of studying arabic sign language \n",
      "PREDICTED: words of the day are scattered in religion \n",
      "\n",
      "ACTUAL:    the subject of studying arabic sign language \n",
      "PREDICTED: the subject of studying arabic sign language \n",
      "\n",
      "ACTUAL:    god does not shirk \n",
      "PREDICTED: the i of to arabic another language \n",
      "\n",
      "ACTUAL:    god is the greatest \n",
      "PREDICTED: god does not shirk \n",
      "\n",
      "ACTUAL:    all deaf arab listeners \n",
      "PREDICTED: words of the day are scattered in religion \n",
      "\n",
      "ACTUAL:    god name \n",
      "PREDICTED: words of the day are scattered in religion \n",
      "\n",
      "ACTUAL:    today i present to you another program \n",
      "PREDICTED: today i present to you another program \n",
      "\n",
      "ACTUAL:    god does not shirk \n",
      "PREDICTED: words of the day are scattered in religion \n",
      "\n",
      "ACTUAL:    thank god \n",
      "PREDICTED: the subject of studying arabic sign language \n",
      "\n",
      "ACTUAL:    also normal words \n",
      "PREDICTED: the subject of studying arabic sign language \n",
      "\n",
      "ACTUAL:    the subject of studying arabic sign language \n",
      "PREDICTED: the subject of studying arabic sign language \n",
      "\n",
      "ACTUAL:    today i present to you another program \n",
      "PREDICTED: words of the day are scattered in religion \n",
      "\n",
      "ACTUAL:    god name \n",
      "PREDICTED: peace be upon you may gods mercy and blessings be upon you \n",
      "\n",
      "ACTUAL:    words of the day are scattered in religion \n",
      "PREDICTED: words of the day are scattered in religion \n",
      "\n",
      "ACTUAL:    thank god \n",
      "PREDICTED: peace be upon you may gods mercy and blessings be upon you \n",
      "\n",
      "ACTUAL:    thank god \n",
      "PREDICTED: today i present to you another program \n",
      "\n",
      "ACTUAL:    god does not shirk \n",
      "PREDICTED: words of the day are scattered in religion \n",
      "\n",
      "ACTUAL:    today i present to you another program \n",
      "PREDICTED: today i present to you another program \n",
      "\n",
      "ACTUAL:    the subject of studying arabic sign language \n",
      "PREDICTED: today i present to you another program \n",
      "\n",
      "ACTUAL:    also normal words \n",
      "PREDICTED: the of of studying arabic sign language \n",
      "\n",
      "ACTUAL:    thank god \n",
      "PREDICTED: peace be upon you may gods mercy and blessings be upon you \n",
      "\n",
      "ACTUAL:    all deaf arab listeners \n",
      "PREDICTED: today i present to you another program \n",
      "\n",
      "ACTUAL:    also normal words \n",
      "PREDICTED: words of the day are scattered in religion \n",
      "\n",
      "ACTUAL:    god is the greatest \n",
      "PREDICTED: all deaf arab listeners \n",
      "\n",
      "ACTUAL:    peace be upon you may gods mercy and blessings be upon you \n",
      "PREDICTED: today i present to you another program \n",
      "\n",
      "ACTUAL:    all deaf arab listeners \n",
      "PREDICTED: words of the day are scattered in religion \n",
      "\n",
      "ACTUAL:    words of the day are scattered in religion \n",
      "PREDICTED: words of the day are scattered in religion \n",
      "\n",
      "ACTUAL:    god is the greatest \n",
      "PREDICTED: god does not shirk \n",
      "\n",
      "ACTUAL:    thank god \n",
      "PREDICTED: words of the day are scattered in religion \n",
      "\n",
      "ACTUAL:    god does not shirk \n",
      "PREDICTED: words of the day are scattered in religion \n",
      "\n",
      "ACTUAL:    the subject of studying arabic sign language \n",
      "PREDICTED: today i present to you another program \n",
      "\n",
      "ACTUAL:    also normal words \n",
      "PREDICTED: the of of studying arabic sign language \n",
      "\n",
      "ACTUAL:    the subject of studying arabic sign language \n",
      "PREDICTED: the subject of studying arabic sign language \n",
      "\n",
      "ACTUAL:    today i present to you another program \n",
      "PREDICTED: today i present to you another program \n",
      "\n",
      "ACTUAL:    today i present to you another program \n",
      "PREDICTED: today i present to you another program \n",
      "\n",
      "ACTUAL:    words of the day are scattered in religion \n",
      "PREDICTED: words of the day are scattered in religion \n",
      "\n",
      "ACTUAL:    god does not shirk \n",
      "PREDICTED: words of the day are scattered in religion \n",
      "\n",
      "WER = 1.0611110925674438\n"
     ]
    }
   ],
   "source": [
    "test_the_model(test_loader, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hence, we have come to an end of this assignment.  \n",
    "We have seen an encoder-decoder model without attention and with attention.  \n",
    "We have also seen the WER of both the models - which are pretty good. The main reason for this good error rate is the number of sentences to predict.  \n",
    "The captions are just 10. However, we can increase the dataset quality to attain more videos and captions and then evaluate the model architecture.  \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.4 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "faafacdf547457256517978ccd617e5232c831db2db3475f4239736a0901b48f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
